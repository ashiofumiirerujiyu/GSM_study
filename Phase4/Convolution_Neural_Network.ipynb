{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fix Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# 시드 고정 함수 정의\n",
    "def seed_everything(seed=5):\n",
    "    # Python의 기본 random 모듈 시드 설정\n",
    "    random.seed(seed)\n",
    "    \n",
    "    # NumPy의 난수 생성 시드 설정\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # PyTorch의 CPU 난수 생성 시드 설정\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    # CUDA가 사용 가능한 경우 GPU 난수 생성 시드 설정\n",
    "    if torch.cuda.is_available():\n",
    "        # 현재 GPU 장치에 대한 시드 설정\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        # 모든 GPU 장치에 대한 시드 설정\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    # PyTorch의 CuDNN 설정을 통해 재현성을 보장\n",
    "    # CuDNN이 제공하는 최적화가 정확하게 재현될 수 있도록 보장\n",
    "    torch.backends.cudnn.deterministic = True  # 고정된 알고리즘만 사용하도록 설정\n",
    "    torch.backends.cudnn.benchmark = False  # 입력 크기가 고정된 경우 최적화 비활성화\n",
    "\n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dir_paths, transform=None):\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        self.transform = transform\n",
    "\n",
    "        for dir_path in dir_paths:\n",
    "            label = os.path.basename(dir_path) # 'cat' 또는 'dog'\n",
    "            for file_name in os.listdir(dir_path):\n",
    "                image_path = os.path.join(dir_path, file_name)\n",
    "                image = Image.open(image_path).convert('RGB')\n",
    "                if self.transform:\n",
    "                    image = self.transform(image)\n",
    "                self.images.append(image)\n",
    "                self.labels.append(label)\n",
    "                \n",
    "        # 레이블 인코딩\n",
    "        self.label_to_index = {'cat': 0, 'dog': 1}\n",
    "        self.labels = [self.label_to_index[label] for label in self.labels]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        image = image.float() / 255.0  # Normalize to [0, 1]\n",
    "\n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 배치 이미지 크기: torch.Size([16, 3, 32, 32])\n",
      "학습 배치 레이블 크기: torch.Size([16])\n",
      "검증 배치 이미지 크기: torch.Size([16, 3, 32, 32])\n",
      "검증 배치 레이블 크기: torch.Size([16])\n",
      "테스트 배치 이미지 크기: torch.Size([16, 3, 32, 32])\n",
      "테스트 배치 레이블 크기: torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "# 데이터 전처리 정의 (Data Augmentation 추가)\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomRotation(45),\n",
    "    transforms.RandomResizedCrop(32, scale=(0.5, 1.0)),\n",
    "    transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.3),\n",
    "    transforms.RandomGrayscale(p=0.3),\n",
    "    transforms.GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 5)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# 디렉터리 경로\n",
    "cat_dir = '../data/cifar10_images/cat'\n",
    "dog_dir = '../data/cifar10_images/dog'\n",
    "\n",
    "# Custom Dataset 인스턴스 생성\n",
    "dataset = CustomDataset(dir_paths=[cat_dir, dog_dir], transform=transform)\n",
    "\n",
    "# 데이터셋을 학습용, 검증용, 테스트용으로 분리\n",
    "train_size = int(0.7 * len(dataset))\n",
    "valid_size = int(0.15 * len(dataset))\n",
    "test_size = len(dataset) - train_size - valid_size\n",
    "train_dataset, valid_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, valid_size, test_size])\n",
    "\n",
    "# DataLoader 인스턴스 생성\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 데이터 확인\n",
    "for images, labels in train_loader:\n",
    "    print(f\"학습 배치 이미지 크기: {images.size()}\")\n",
    "    print(f\"학습 배치 레이블 크기: {labels.size()}\")\n",
    "    break\n",
    "\n",
    "for images, labels in valid_loader:\n",
    "    print(f\"검증 배치 이미지 크기: {images.size()}\")\n",
    "    print(f\"검증 배치 레이블 크기: {labels.size()}\")\n",
    "    break\n",
    "\n",
    "for images, labels in test_loader:\n",
    "    print(f\"테스트 배치 이미지 크기: {images.size()}\")\n",
    "    print(f\"테스트 배치 레이블 크기: {labels.size()}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HyperParameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼파라미터 설정\n",
    "learning_rate = 0.0001\n",
    "num_epochs = 100\n",
    "weight_decay = 1e-5  # L2 정규화 하이퍼파라미터\n",
    "early_stopping_patience = 100\n",
    "early_stopping_counter = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weight Initalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# 가중치 초기화 함수\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "    elif isinstance(m, nn.Linear):\n",
    "        nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class EnhancedCNN(nn.Module):\n",
    "    def __init__(self, input_channels, num_classes):\n",
    "        super(EnhancedCNN, self).__init__()\n",
    "        \n",
    "        # Convolutional Block 1\n",
    "        self.conv1 = nn.Conv2d(in_channels=input_channels, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        \n",
    "        # Convolutional Block 2\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.conv4 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(256)\n",
    "        \n",
    "        # Convolutional Block 3\n",
    "        self.conv5 = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn5 = nn.BatchNorm2d(512)\n",
    "        \n",
    "        # Fully Connected Layers\n",
    "        self.fc1 = nn.Linear(512 * 4 * 4, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.fc3 = nn.Linear(512, num_classes)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.apply(weights_init)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Convolutional Block 1\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        # Convolutional Block 2\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = F.relu(self.bn4(self.conv4(x)))\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        # Convolutional Block 3\n",
    "        x = F.relu(self.bn5(self.conv5(x)))\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        # Flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Fully Connected Layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return torch.sigmoid(x)\n",
    "\n",
    "model = EnhancedCNN(input_channels=3, num_classes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight: 864 parameters\n",
      "conv1.bias: 32 parameters\n",
      "bn1.weight: 32 parameters\n",
      "bn1.bias: 32 parameters\n",
      "conv2.weight: 18432 parameters\n",
      "conv2.bias: 64 parameters\n",
      "bn2.weight: 64 parameters\n",
      "bn2.bias: 64 parameters\n",
      "conv3.weight: 73728 parameters\n",
      "conv3.bias: 128 parameters\n",
      "bn3.weight: 128 parameters\n",
      "bn3.bias: 128 parameters\n",
      "conv4.weight: 294912 parameters\n",
      "conv4.bias: 256 parameters\n",
      "bn4.weight: 256 parameters\n",
      "bn4.bias: 256 parameters\n",
      "conv5.weight: 1179648 parameters\n",
      "conv5.bias: 512 parameters\n",
      "bn5.weight: 512 parameters\n",
      "bn5.bias: 512 parameters\n",
      "fc1.weight: 8388608 parameters\n",
      "fc1.bias: 1024 parameters\n",
      "fc2.weight: 524288 parameters\n",
      "fc2.bias: 512 parameters\n",
      "fc3.weight: 512 parameters\n",
      "fc3.bias: 1 parameters\n",
      "Total parameters: 10485505\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    \"\"\"\n",
    "    모델의 파라미터 수를 계산하는 함수\n",
    "    \"\"\"\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    return total_params\n",
    "\n",
    "def print_model_parameters(model):\n",
    "    \"\"\"\n",
    "    모델의 각 레이어와 그 파라미터 수를 출력하는 함수\n",
    "    \"\"\"\n",
    "    for name, param in model.named_parameters():\n",
    "        print(f\"{name}: {param.numel()} parameters\")\n",
    "    print(f\"Total parameters: {count_parameters(model)}\")\n",
    "\n",
    "# 모델의 파라미터 수 출력\n",
    "print_model_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\begin{align}\n",
    "-\\frac{1}{N}\\sum_{i=1}^N[y_ilog(p_i)+(1-y_i)log(1-p_i)]\n",
    "\\end{align}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 선형 모델의 출력: [0.5, -1.2, 0.8]\n",
    "2. Sigmoid의 출력: [0.62245933, 0.23147522, 0.68997448]\n",
    "3. 실제 레이블: [1, 0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이진 크로스 엔트로피 손실 함수\n",
    "# 모델의 예측 확률과 실제 레이블 간의 차이를 측정하여, 예측이 정확할수록 손실을 줄이는 방향으로 모델을 학습시킵니다.\n",
    "criterion = nn.BCELoss()  # Binary Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Adam 옵티마이저 사용, L2 정규화 포함\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 1.5398\n",
      "Epoch 1/100, Validation Loss: 0.6961\n",
      "Epoch 2/100, Loss: 1.2770\n",
      "Epoch 2/100, Validation Loss: 0.7287\n",
      "Epoch 3/100, Loss: 1.1225\n",
      "Epoch 3/100, Validation Loss: 0.7759\n",
      "Epoch 4/100, Loss: 0.9381\n",
      "Epoch 4/100, Validation Loss: 0.7566\n",
      "Epoch 5/100, Loss: 0.9452\n",
      "Epoch 5/100, Validation Loss: 0.7325\n",
      "Epoch 6/100, Loss: 0.8602\n",
      "Epoch 6/100, Validation Loss: 0.6732\n",
      "Epoch 7/100, Loss: 0.9171\n",
      "Epoch 7/100, Validation Loss: 0.6958\n",
      "Epoch 8/100, Loss: 0.8084\n",
      "Epoch 8/100, Validation Loss: 0.6838\n",
      "Epoch 9/100, Loss: 0.8066\n",
      "Epoch 9/100, Validation Loss: 0.6964\n",
      "Epoch 10/100, Loss: 0.7707\n",
      "Epoch 10/100, Validation Loss: 0.6931\n",
      "Epoch 11/100, Loss: 0.7780\n",
      "Epoch 11/100, Validation Loss: 0.6925\n",
      "Epoch 12/100, Loss: 0.7677\n",
      "Epoch 12/100, Validation Loss: 0.7027\n",
      "Epoch 13/100, Loss: 0.7096\n",
      "Epoch 13/100, Validation Loss: 0.7157\n",
      "Epoch 14/100, Loss: 0.7407\n",
      "Epoch 14/100, Validation Loss: 0.6980\n",
      "Epoch 15/100, Loss: 0.6790\n",
      "Epoch 15/100, Validation Loss: 0.6850\n",
      "Epoch 16/100, Loss: 0.6586\n",
      "Epoch 16/100, Validation Loss: 0.6898\n",
      "Epoch 17/100, Loss: 0.6622\n",
      "Epoch 17/100, Validation Loss: 0.6884\n",
      "Epoch 18/100, Loss: 0.6463\n",
      "Epoch 18/100, Validation Loss: 0.6959\n",
      "Epoch 19/100, Loss: 0.6516\n",
      "Epoch 19/100, Validation Loss: 0.6835\n",
      "Epoch 20/100, Loss: 0.6607\n",
      "Epoch 20/100, Validation Loss: 0.6921\n",
      "Epoch 21/100, Loss: 0.6315\n",
      "Epoch 21/100, Validation Loss: 0.6996\n",
      "Epoch 22/100, Loss: 0.6063\n",
      "Epoch 22/100, Validation Loss: 0.7130\n",
      "Epoch 23/100, Loss: 0.6036\n",
      "Epoch 23/100, Validation Loss: 0.7077\n",
      "Epoch 24/100, Loss: 0.5671\n",
      "Epoch 24/100, Validation Loss: 0.7430\n",
      "Epoch 25/100, Loss: 0.5809\n",
      "Epoch 25/100, Validation Loss: 0.6945\n",
      "Epoch 26/100, Loss: 0.5473\n",
      "Epoch 26/100, Validation Loss: 0.6980\n",
      "Epoch 27/100, Loss: 0.5613\n",
      "Epoch 27/100, Validation Loss: 0.7178\n",
      "Epoch 28/100, Loss: 0.5568\n",
      "Epoch 28/100, Validation Loss: 0.7489\n",
      "Epoch 29/100, Loss: 0.5189\n",
      "Epoch 29/100, Validation Loss: 0.7464\n",
      "Epoch 30/100, Loss: 0.4867\n",
      "Epoch 30/100, Validation Loss: 0.7495\n",
      "Epoch 31/100, Loss: 0.4787\n",
      "Epoch 31/100, Validation Loss: 0.7371\n",
      "Epoch 32/100, Loss: 0.5018\n",
      "Epoch 32/100, Validation Loss: 0.7759\n",
      "Epoch 33/100, Loss: 0.4688\n",
      "Epoch 33/100, Validation Loss: 0.7878\n",
      "Epoch 34/100, Loss: 0.4140\n",
      "Epoch 34/100, Validation Loss: 0.8027\n",
      "Epoch 35/100, Loss: 0.3961\n",
      "Epoch 35/100, Validation Loss: 0.7844\n",
      "Epoch 36/100, Loss: 0.3642\n",
      "Epoch 36/100, Validation Loss: 0.8000\n",
      "Epoch 37/100, Loss: 0.3313\n",
      "Epoch 37/100, Validation Loss: 0.9666\n",
      "Epoch 38/100, Loss: 0.3397\n",
      "Epoch 38/100, Validation Loss: 0.9057\n",
      "Epoch 39/100, Loss: 0.3616\n",
      "Epoch 39/100, Validation Loss: 0.7572\n",
      "Epoch 40/100, Loss: 0.3274\n",
      "Epoch 40/100, Validation Loss: 0.8967\n",
      "Epoch 41/100, Loss: 0.2863\n",
      "Epoch 41/100, Validation Loss: 0.8566\n",
      "Epoch 42/100, Loss: 0.2924\n",
      "Epoch 42/100, Validation Loss: 1.1189\n",
      "Epoch 43/100, Loss: 0.2725\n",
      "Epoch 43/100, Validation Loss: 0.9257\n",
      "Epoch 44/100, Loss: 0.2301\n",
      "Epoch 44/100, Validation Loss: 0.9271\n",
      "Epoch 45/100, Loss: 0.2311\n",
      "Epoch 45/100, Validation Loss: 0.9741\n",
      "Epoch 46/100, Loss: 0.2255\n",
      "Epoch 46/100, Validation Loss: 1.2846\n",
      "Epoch 47/100, Loss: 0.2793\n",
      "Epoch 47/100, Validation Loss: 0.9601\n",
      "Epoch 48/100, Loss: 0.1976\n",
      "Epoch 48/100, Validation Loss: 1.3052\n",
      "Epoch 49/100, Loss: 0.1780\n",
      "Epoch 49/100, Validation Loss: 1.1084\n",
      "Epoch 50/100, Loss: 0.1881\n",
      "Epoch 50/100, Validation Loss: 1.5161\n",
      "Epoch 51/100, Loss: 0.1828\n",
      "Epoch 51/100, Validation Loss: 1.0586\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     22\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 23\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     26\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m images\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:768\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    766\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 768\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    769\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    770\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    771\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    772\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "# 조기 종료 설정\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "# 학습 및 검증 손실을 저장할 리스트 초기화\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "\n",
    "# 학습 루프 시작\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        labels = labels.float().view(-1, 1)\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    train_losses.append(epoch_loss)\n",
    "    \n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss:.4f}')\n",
    "\n",
    "    model.eval()\n",
    "    val_running_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in valid_loader:\n",
    "            labels = labels.float().view(-1, 1)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_running_loss += loss.item() * images.size(0)\n",
    "\n",
    "    val_epoch_loss = val_running_loss / len(valid_loader.dataset)\n",
    "    valid_losses.append(val_epoch_loss)\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}, Validation Loss: {val_epoch_loss:.4f}')\n",
    "\n",
    "    if val_epoch_loss < best_val_loss:\n",
    "        best_val_loss = val_epoch_loss\n",
    "        early_stopping_counter = 0\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "    else:\n",
    "        early_stopping_counter += 1\n",
    "\n",
    "    if early_stopping_counter >= early_stopping_patience:\n",
    "        print(\"Early stopping\")\n",
    "        break\n",
    "\n",
    "# 학습 및 검증 손실 시각화\n",
    "epochs_range = range(1, len(train_losses) + 1)  # 실제 기록된 손실의 길이에 맞게 조정\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(epochs_range, train_losses, label='Train Loss')\n",
    "plt.plot(epochs_range, valid_losses, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델을 평가 모드로 설정\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# 평가 중 기울기 계산을 비활성화 (메모리 절약 및 연산 속도 향상)\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        labels = labels.float().view(-1, 1)\n",
    "        outputs = model(images)\n",
    "        predicted = (outputs > 0.5).float()\n",
    "\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "# 정확도 계산\n",
    "accuracy = correct / total\n",
    "# 정확도를 백분율로 출력\n",
    "print(f\"Test Set Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
