{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_image_path = \"../data/cifar10_images/cat\"\n",
    "dog_image_path = \"../data/cifar10_images/dog\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_from_path(path):\n",
    "    images = []\n",
    "    labels = []\n",
    "    label = path.split('/')[-1]\n",
    "    for file_name in os.listdir(path):\n",
    "        image_path = os.path.join(path, file_name)\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.resize(image, (32, 32))\n",
    "        image = image.reshape(-1)\n",
    "        image = image / 255.0\n",
    "        images.append(image)\n",
    "\n",
    "        if label == 'cat':\n",
    "            labels.append(0)\n",
    "        elif label == 'dog':\n",
    "            labels.append(1)\n",
    "        else:\n",
    "            raise ValueError(f\"Unexpected label: {label}. Expected 'cat' or 'dog'.\")\n",
    "\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape: (3072,)\n",
      "label shape: 0\n",
      "number of cat data: 500\n",
      "number of dog data: 500\n"
     ]
    }
   ],
   "source": [
    "cat_images, cat_labels = load_image_from_path(cat_image_path)\n",
    "dog_images, dog_labels = load_image_from_path(dog_image_path)\n",
    "\n",
    "print(f\"data shape: {cat_images[0].shape}\")\n",
    "print(f\"label shape: {cat_labels[0]}\")\n",
    "print(f\"number of cat data: {len(cat_labels)}\")\n",
    "print(f\"number of dog data: {len(dog_labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of all data: 1000\n"
     ]
    }
   ],
   "source": [
    "images = np.array(cat_images + dog_images)\n",
    "labels = np.array(cat_labels + dog_labels)\n",
    "\n",
    "print(f\"number of all data: {len(labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of train data: 800\n",
      "number of test data: 200\n",
      "check data set randomly: [1 1 1 1 0 1 1 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "train_x, test_x, train_y, test_y = train_test_split(images, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"number of train data: {len(train_y)}\")\n",
    "print(f\"number of test data: {len(test_y)}\")\n",
    "print(f\"check data set randomly: {test_y[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = train_y.reshape((800, 1))\n",
    "test_y = test_y.reshape((200, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HyperParameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 3072\n",
    "num_samples = 800\n",
    "learning_rate = 0.1\n",
    "num_iterations = 1000\n",
    "hidden_layer_size = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = np.random.randn(num_features, hidden_layer_size) * 0.01\n",
    "b1 = np.zeros((1, hidden_layer_size))\n",
    "W2 = np.random.randn(hidden_layer_size, 1) * 0.01\n",
    "b2 = np.zeros((1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_cross_entropy(y_true, y_pred):\n",
    "    epsilon = 1e-15\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)  # Numerical stability\n",
    "    loss = -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(z):\n",
    "    return np.maximum(0, z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Loss: 0.6933912525141142\n",
      "Iteration 100, Loss: 0.6595873309225564\n",
      "Iteration 200, Loss: 0.7019586885822343\n",
      "Iteration 300, Loss: 0.5638684578754173\n",
      "Iteration 400, Loss: 0.5523438212703811\n",
      "Iteration 500, Loss: 0.6274251174498421\n",
      "Iteration 600, Loss: 0.49719117831275866\n",
      "Iteration 700, Loss: 0.6466506685038893\n",
      "Iteration 800, Loss: 0.3880684014564935\n",
      "Iteration 900, Loss: 0.5639808901190831\n",
      "Final Loss: 0.5535857684869286\n"
     ]
    }
   ],
   "source": [
    "for i in range(num_iterations):\n",
    "    Z1 = np.dot(train_x, W1) + b1\n",
    "    A1 = relu(Z1)\n",
    "    Z2 = np.dot(A1, W2) + b2\n",
    "    y_hat = sigmoid(Z2)\n",
    "\n",
    "    loss = binary_cross_entropy(train_y, y_hat)\n",
    "\n",
    "    dZ2 = y_hat - train_y\n",
    "    dW2 = np.dot(A1.T, dZ2) / num_samples\n",
    "    db2 = np.sum(dZ2, axis=0, keepdims=True) / num_samples\n",
    "    \n",
    "    dA1 = np.dot(dZ2, W2.T)\n",
    "    dZ1 = dA1 * (Z1 > 0)\n",
    "    dW1 = np.dot(train_x.T, dZ1) / num_samples\n",
    "    db1 = np.sum(dZ1, axis=0, keepdims=True) / num_samples\n",
    "\n",
    "    W1 -= learning_rate * dW1\n",
    "    b1 -= learning_rate * db1\n",
    "    W2 -= learning_rate * dW2\n",
    "    b2 -= learning_rate * db2\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print(f\"Iteration {i}, Loss: {loss}\")\n",
    "\n",
    "print(f\"Final Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Accuracy: 59.50%\n"
     ]
    }
   ],
   "source": [
    "Z1_test = np.dot(test_x, W1) + b1\n",
    "A1_test = relu(Z1_test)\n",
    "Z2_test = np.dot(A1_test, W2) + b2\n",
    "test_y_hat = sigmoid(Z2_test)\n",
    "\n",
    "test_y_pred = (test_y_hat > 0.5).astype(int)\n",
    "accuracy = np.mean(test_y_pred == test_y)\n",
    "\n",
    "print(f\"Test Set Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
